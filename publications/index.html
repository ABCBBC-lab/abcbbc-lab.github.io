<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | ABCBBC Lab </title> <meta name="author" content="animal behavior, computational bioacoustics, brains and cognition "> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="cognition, comparative psychology, animal behavior, computational bioacoustics, brains, cognition"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://abcbbc-lab.github.io/publications/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> ABCBBC Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <script src="/assets/js/bibsearch.js?v=1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2026</h2> <ol class="bibliography"> <li> <div class="row"> <div id="bauerExploringMarineMammal2026" class="col-sm-10"> <div class="title">Exploring Marine Mammal Cognition as a Conservation Tool</div> <div class="author"> Gordon B. Bauer, Peter F. Cook, Heidi E. Harley, and <span class="more-authors" title="click to view 19 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '19 more authors' ? 'Jason Bruck, Mel Cosentino, Charles J. Deutsch, Nicola Erdsack, Wendi Fellner, Tabitha Gunnars, Heather Manitzas Hill, Sonia V. Kumar, Malin K. Lilley, Katherine A. McHugh, Jennifer Moore, Juliana R. Moron, Andrea Ravignani, Roger L. Reep, Athena M. Rycyk, Laela S. Sayigh, Anikó Szegedi, Christina Toms, Randall S. Wells' : '19 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">19 more authors</span> </div> <div class="periodical"> <em>Marine Mammal Science</em>, Jan 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/mms.70114" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>ABSTRACT Cognition is an animal’s real-time adaptation system for responding to change. Rapid environmental change, often anthropogenic, is expanding the range and severity of challenges confronting wild animals. Effective conservation requires a multifaceted approach that includes animals’ capacities. Large-brained, long-lived animals such as marine mammals often have extensive capability to adaptively modify their behavior due to their cognition, which comprises the mechanisms of information acquisition, processing, and flexible action. Consequently, current behavior need not be a final predictor of future behavior for these animals. This flexibility provides an underutilized and under examined point of leverage for humans interested in improving life outcomes for wild animals. In this team-written, interdisciplinary paper, we argue that application of cognitive approaches may facilitate many conservation efforts directed toward marine mammals. Starting with a workshop on this topic at the 24th Biennial Conference on the Biology of Marine Mammals, scientists representing a wide range of disciplinary expertise addressed eight different conservation concerns for six marine mammal species and provided potential cognitive explanations of and interventions aimed at related behavior. Our treatment highlights the value of integrated laboratory and field research, and the importance of tight lines of communication between scientists and conservation managers.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="francesconiSEXBonobosIntensity2026" class="col-sm-10"> <div class="title">SEX in Bonobos: The Intensity of Sexual Stimulation Sharply Drops after Facial Mimicry</div> <div class="author"> Martina Francesconi, Alice Galotti, <em>Yannick Jadoul</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Federico Giovannini, Andrea Ravignani, Elisabetta Palagi' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Evolution and Human Behavior</em>, Jan 2026 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1016/j.evolhumbehav.2025.106786" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div id="giomoGlobalLocalDeviance2026" class="col-sm-10"> <div class="title">Global and Local Deviance Effects in the Processing of Temporal Patterns</div> <div class="author"> <em>Dunia Giomo</em>, Romain Brasselet, Gianfranco Fortunato, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Domenica Bueti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Annals of the New York Academy of Sciences</em>, Feb 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/nyas.70173" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>ABSTRACT Perceptual and sensorimotor events are often experienced as temporal patterns, that is, identified as sequences based on their temporal features. While current timing models propose separate mechanisms supporting the processing of single intervals and temporal patterns, they leave partially unclear whether the latter entails the processing of both individual intervals and the overall structure of a pattern, or only one of these features. Here, we narrowed this question down by investigating how violations of regularity within the individual intervals of a temporal sequence (i.e., local violations) and in its overall structure (i.e., global violations) differentially affect its reproduction. We tested these violation effects in three experiments in which the sequences were experienced either in the visual or auditory domain and had either simple or complex structures. Results showed that the precision in reproducing simple visual and auditory patterns was primarily affected by local violations, whereas global violations mostly impacted the reproduction of visual patterns with complex structures. These detrimental effects were partially explained by rescaling and bias effects in the reproduced patterns. Overall, our findings indicate that the processing and reproduction of temporal patterns differentially weigh individual intervals and global structure, depending on sensory modality and, for visual patterns, on structural complexity. , Our experience of the world is inherently structured by temporal patterns. Yet a full understanding of how we process such patterns is still lacking. Across three finger-tapping experiments employing the local–global paradigm, we show that the processing and reproduction of temporal patterns rely on a differential weighting of their individual elements and overall structure. This weighting depends on both the structural complexity of the patterns and the sensory modality in which they are experienced.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="martinRooksCorvusFrugilegus2026" class="col-sm-10"> <div class="title">Rooks (Corvus Frugilegus) Can Show Spontaneous Vocal Flexibility When Exposed to Dynamically Changing Rhythmic Sounds</div> <div class="author"> K. Martin, M. Tomasek, A. Hivet, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'A. Ravignani, N. Obin, V. Dufour' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Animal Cognition</em>, Jan 2026 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1007/s10071-025-02038-w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div id="raimondiLearntFormantModulation2026" class="col-sm-10"> <div class="title">Learnt Formant Modulation via Upper Vocal Tract Movements in a Marine Mammal</div> <div class="author"> <em>Teresa Raimondi</em>, Francesca D’Orazio, Denise Di Martino, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Melina Witt, Flavia Grenga, Peter Cook, Livio Favaro, Cristina Pilenga, Andrea Ravignani' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Discover Animals</em>, Jan 2026 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1007/s44338-025-00145-z" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div id="raimondiNeonatalAuditoryInput2026" class="col-sm-10"> <div class="title">Neonatal Auditory Input Affects Vocal Development in Harbour Seals</div> <div class="author"> <em>Teresa Raimondi</em>, Caroline E. Haas, Koen De Reus, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Margarita Mendez-Arostegui, Yannick Jadoul, Andrea Ravignani' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Philosophical Transactions B</em>, Feb 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1098/rstb.2024.0369" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Vocal individuality has important biological functions in mammals: at crucial stages of development, it ensures feeding and is a prerequisite for auditory-based mother–pup recognition. Is vocal individuality only shaped by maturation or also by the degree of conspecific acoustic input? Here, we test how the neonatal auditory environment shapes development and individualization of calls in harbour seal pups (Phoca vitulina). To simulate low- versus high-conspecific acoustic density, 18 pups heard playbacks of calls from either 2 or 30 conspecifics. We recorded calls before and after playback exposure and extracted 12 acoustic parameters. Supervised machine learning and discriminant function analyses showed greater individual distinctiveness in both groups after playback, indicating a developmental trend toward individualization. Notably, pups exposed to less-variable input showed higher individuality. Euclidean distances on call parameters showed that both groups diverged from the playback signals. Distances on within-pup and between-pups housed together revealed opposite trajectories in the two groups: after the exposure, less-variable auditory input determined steadier individual calls, while more-variable auditory input scattered calls across the acoustic space. Altogether, our findings indicate that auditory input modulates vocal development in pups, making harbour seals a promising model for unravelling how neonatal environment affects vocal plasticity in a non-human mammal. This article is part of the theme issue ‘Mechanisms of learning from social interaction’.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div id="biancoNeuralEncodingMusical2025" class="col-sm-10"> <div class="title">Neural Encoding of Musical Expectations in a Non-Human Primate</div> <div class="author"> Roberta Bianco, Nathaniel J. Zuk, Félix Bigand, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Eros Quarta, Stefano Grasso, Flavia Arnese, Andrea Ravignani, Alexandra Battaglia-Mayer, Giacomo Novembre' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Current Biology</em>, Feb 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1016/j.cub.2025.01.015" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div id="distefanoTestingHypothesesMajor2025" class="col-sm-10"> <div class="title">Testing Hypotheses on the Major/Minor Dichotomy: Movement, Consonance/Dissonance, and (the Lack of) Cross-Species Evidence. Comment on “the Major-Minor Mode Dichotomy in Music Perception” by Giulio Carraturo, Victor Pando-Naude, Marco Costa, Peter Vuust, Leonardo Bonetti, Elvira Brattico</div> <div class="author"> Nicola Di Stefano and <em>Andrea Ravignani</em> </div> <div class="periodical"> <em>Physics of Life Reviews</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1016/j.plrev.2025.05.001" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div id="duengenHarborSealPhoca2025" class="col-sm-10"> <div class="title">A Harbor Seal (Phoca Vitulina) Shows Extensive Respiratory Control in Sound Production</div> <div class="author"> Diandra Duengen, <em>Yannick Jadoul</em>, and <em>Andrea Ravignani</em> </div> <div class="periodical"> <em>BMC Ecology and Evolution</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1186/s12862-025-02404-9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div id="duengenTrainingExperimentallyNaive2024" class="col-sm-10"> <div class="title">Training Experimentally Naive Seals for Vocal Learning Experiments</div> <div class="author"> Diandra Duengen and <em>Andrea Ravignani</em> </div> <div class="periodical"> Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1101/2024.08.27.609954" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Harbor seals (Phoca vitulina) are a common zoo species that show a scientifically valuable propensity for vocal learning. Under human care, the seals can be trained to associate vocalizations with cues. This ability is termed vocal usage learning and is characterized by learning to use a vocalization in a specific context. Among mammals, seals are prime candidates to investigate vocal learning. Yet, only a handful of reports exist on harbor seal vocal learning abilities, and even fewer document how these were trained or tested. Here, we investigate how, and if, two experimentally naive harbor seals under human care can be trained to participate in scientific experiments. We describe the training and testing of two seals in two basic vocal learning experiments. We trained the animals to vocalize upon the presentation of discriminative stimuli (SD) through operant conditioning methods and tested their abilities to i) vocalize and refrain from vocalizing on two distinct SD’s, and ii) produce two different vocalizations upon the presentation of two different SD’s. Both seals learned the tasks: the first task was achieved within 118 trials (22 errors to criterion) and 220 trials (40 errors to criterion), the second task within 480 trials (158 errors to criterion) and 380 trials (94 errors to criterion), respectively. Our results confirm that harbor seals are capable of vocal usage learning and further suggest that associating individually distinct vocalizations with different SD’s may be more cognitively demanding than vocalizing and being silent on SD.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="eleuteriChimpanzeeDrummingShows2025" class="col-sm-10"> <div class="title">Chimpanzee Drumming Shows Rhythmicity and Subspecies Variation</div> <div class="author"> Vesta Eleuteri, <em>Jelle van der Werff</em>, Wytse Wilhelm, and <span class="more-authors" title="click to view 18 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '18 more authors' ? 'Adrian Soldati, Catherine Crockford, Nisarg Desai, Pawel Fedurek, Maegan Fitzgerald, Kirsty E. Graham, Kathelijne Koops, Jill Pruetz, Liran Samuni, Katie Slocombe, Angela Stoeger, Michael L. Wilson, Roman M. Wittig, Klaus Zuberbühler, Henry D. Camara, Gnan Mamy, Andrea Ravignani, Catherine Hobaiter' : '18 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">18 more authors</span> </div> <div class="periodical"> <em>Current Biology</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1016/j.cub.2025.04.019" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div id="hershAccelerandoCrescendoAfrican2025" class="col-sm-10"> <div class="title">Accelerando and Crescendo in African Penguin Ecstatic Display Songs</div> <div class="author"> Taylor A. Hersh, <em>Yannick Jadoul</em>, Marco Gamba, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Andrea Ravignani, Livio Favaro' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Annals of the New York Academy of Sciences</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/nyas.15383" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Many species produce rhythmic sound sequences. Some purportedly speed up their vocalizations throughout a display, reminiscent of—but not necessarily equivalent to— accelerando in human music. This phenomenon has been frequently reported but rarely quantified, which limits our ability to understand its mechanism, function, and evolution. Here, we use a suite of rhythm analyses to quantify temporal and acoustic features in the ecstatic display songs of male African penguins ( Spheniscus demersus ). We show that songs get faster (i.e., accelerando) and louder (i.e., crescendo) as they progress. The accelerando occurs because the intersyllable silences, not the syllables themselves, predictably shorten over time. This rhythmicity is maintained even when individuals take audible breaths. Individuals also show plasticity: when they start with a slow tempo, they speed up more strongly than when they start with a fast tempo. We hypothesize that this well-timed accelerando may stem from arousal-based mechanisms, biomechanical constraints, or more complex rhythmic control. Future work should test the mechanisms behind this intra-individual rhythmic variation since nonpasserine birds are thought to have limited vocal plasticity. By integrating a rich empirical dataset with cutting-edge rhythm analyses, we establish the necessary foundation to determine how such features evolved and their role(s) across communication systems.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="jadoulEvolutionaryModelRhythmic2025" class="col-sm-10"> <div class="title">An Evolutionary Model of Rhythmic Accelerando in Animal Vocal Signalling</div> <div class="author"> <em>Yannick Jadoul</em>, Taylor A. Hersh, Elias Fernández Domingos, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Marco Gamba, Livio Favaro, Andrea Ravignani' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>PLOS Computational Biology</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1371/journal.pcbi.1013011" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Animal acoustic communication contains many structural features. Among these, temporal structure, or rhythmicity, is increasingly tested empirically and modelled quantitatively. Accelerando is a rhythmic structure which consists of temporal intervals increasing in rate over a sequence. Why this particular vocal behaviour is widespread in many different animal lineages, and how it evolved, is so far unknown. Here, we use evolutionary game theory and computer simulations to link two rhythmic aspects of animal communication, acceleration and overlap: We test whether rhythmic accelerando could evolve under a pressure for acoustic overlap in time. Our models show that higher acceleration values result in a higher payoff, driven by the higher relative overlap between sequences. The addition of a cost to the payoff matrix models a physiological disadvantage to high acceleration rates and introduces a divergence between an individual’s incentive and the overall payoff of the population. Analysis of the invasion dynamics of acceleration strategies shows a stable, non-invadable range of strategies for moderate acceleration levels. Our computational simulations confirm these results: A simple selective pressure to maximise the expected overlap, while minimising the associated physiological cost, causes an initially isochronous population to evolve towards producing increasingly accelerating sequences until a population-wide equilibrium of rhythmic accelerando is reached. These results are robust to a broad range of parameter values. Overall, our analyses show that if overlap is beneficial, emergent evolutionary dynamics allow a population to gradually start producing accelerating sequences and reach a stable state of moderate acceleration. Finally, our modelling results closely match empirical data recorded from an avian species showing rhythmic accelerando, the African penguin. This shows the productive interplay between theoretical and empirical biology.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="jadoulHiddenAssumptionsInteger2025a" class="col-sm-10"> <div class="title">Hidden Assumptions of Integer Ratio Analyses in Bioacoustics and Music</div> <div class="author"> <em>Yannick Jadoul</em>, Tommaso Tufarelli, <em>Chloé Coissac</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Marco Gamba, Andrea Ravignani' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Annals of the New York Academy of Sciences</em>, Nov 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/nyas.70037" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Rhythm is ubiquitous in human culture and in nature, but hard to capture in all its complexity. A key dimension of rhythm, integer ratio categories occur when the relationship between temporal intervals can be expressed as small-integer ratios. Recent work has found integer ratio categories in most human musical cultures and some animal species’ vocalizations or behavioral displays. But biological systems are noisy, and empirically measured intervals rarely form an exact small-integer ratio. Here, we mathematically assess whether a leading integer ratio analysis method makes valid statistical and biological assumptions. In particular, we (1) make the temporal properties of empirical ratios explicit, both in general and for the typical use in the literature; (2) show how the choice of ratio formula affects the probability distribution of rhythm ratios and ensuing statistical results; (3) guide the reader to carefully consider the assumptions and null hypotheses of the statistical analysis; and (4) present a comprehensive methodology to statistically test integer ratios for any null hypothesis of choice. Our observations have implications for both past and future research in music cognition and animal behavior: They suggest how to interpret past findings and provide tools to choose the correct null hypotheses in future empirical work.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="jadoulRhythmicAnalysisAnimal2025" class="col-sm-10"> <div class="title">Rhythmic Analysis in Animal Communication, Speech, and Music: The Normalized Pairwise Variability Index Is a Summary Statistic of Rhythm Ratios</div> <div class="author"> <em>Yannick Jadoul</em>, Francesca D’Orazio, Vesta Eleuteri, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Jelle van der Werff, Tommaso Tufarelli, Marco Gamba, Teresa Raimondi, Andrea Ravignani' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Vibration</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3390/vibration8020012" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Rhythm is fundamental in many physical and biological systems. Rhythm is relevant to a broad range of phenomena across different fields, including animal bioacoustics, speech sciences, and music cognition. As a result, the interest in developing consistent quantitative measures for cross-disciplinary rhythmic analysis is growing. Two quantitative measures that can be directly applied to any temporal structure are the normalized pairwise variability index (nPVI) and rhythm ratios (rk). The nPVI summarizes the overall isochrony of a sequence, i.e., how regularly spaced a sequence’s events are, as a single value. Meanwhile, rk quantifies ratios between a sequence’s adjacent intervals and is often used for identifying rhythmic categories. Here, we show that these two rhythmic measures are fundamentally connected: the nPVI is a summary static of the rk values of a temporal sequence. This result offers a deeper understanding of how these measures are applied. It also opens the door for creating novel, custom measures to quantify rhythmic patterns based on a sequence’s rk distribution and compare rhythmic patterns across different domains. The explicit connection between nPVI and rk is one further step towards a common quantitative toolkit for rhythm research across disciplines.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="laffiRhythmHorseGaits2025" class="col-sm-10"> <div class="title">The Rhythm of Horse Gaits</div> <div class="author"> Lia Laffi, <em>Teresa Raimondi</em>, Carola Ferrante, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Eleonora Pagliara, Andrea Bertuglia, Elodie Floriane Briefer, Marco Gamba, Andrea Ravignani' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Annals of the New York Academy of Sciences</em>, Jan 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/nyas.15271" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract What makes animal gaits so audibly rhythmic? To answer this question, we recorded the footfall sound of 19 horses and quantified the rhythmic differences in the temporal structure of three natural gaits: walk, trot, and canter. Our analyses show that each gait displays a strikingly specific rhythmic pattern and that all gaits are organized according to small-integer ratios, those found when adjacent temporal intervals are related by a mathematically simple relationship of integer numbers. Walk and trot exhibit an isochronous structure (1:1)—similar to a ticking clock—while canter is characterized by three small-integer ratios (1:1, 1:2, 2:1). While walk and trot both show isochrony, trot has a slower tempo and is more precise and accurate, like a metronome. Our results quantitatively discriminate horse gaits based on rhythm, revealing striking commonalities with human music and some animal communicative signals. Gait and vocal rhythmicity share key features, and the former likely predates the latter; we suggest this supports gait-based hypotheses for the evolution of rhythm. Specifically, the perception of locomotor rhythmicity may have evolved in different species under pressure for predator recognition and mate selection; it may have been later exapted for rhythmic vocal communication.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="laffiRhythmicCategoriesHorse2025" class="col-sm-10"> <div class="title">Rhythmic Categories in Horse Gait Kinematics</div> <div class="author"> Lia Laffi, Félix Bigand, Christian Peham, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Giacomo Novembre, Marco Gamba, Andrea Ravignani' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Journal of Anatomy</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/joa.14200" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Anecdotally, horses’ gaits sound rhythmic. Are they really? In this study, we quantified the motor rhythmicity of horses across three different gaits (walk, trot, and canter). For the first time, we adopted quantitative tools from bioacoustics and music cognition to quantify locomotor rhythmicity. Specifically, we tested whether kinematics data contained rhythmic categories; these occur when adjacent temporal intervals are categorically, rather than randomly, distributed. We extracted the motion cycle duration (t k ) of two ipsilateral hooves from motion data of 13 ridden horses and calculated the ratios from two successive t k values. We tested whether these ratios significantly fell within rhythmic categories and quantified how close they were to small-integer ratios, a rhythmic feature also present in animal vocalizations and human music. We found a strong isochronous pattern—a 1:1 rhythmic ratio, corresponding to the ticking of a clock—in the motion of single limbs for all gaits. We also analyzed the interlimb coordination of the two ipsilateral hooves’ impacts to identify differences associated with the biomechanical patterns of the three gaits. We found an interlimb 1:1 rhythmic pattern for trot and 1:3 and 3:1 rhythmic categories for walk and canter. Our findings are a first step toward quantifying rhythmicity in horse locomotion and potentially the resulting rhythmic sounds, with possible implications as tools to detect gait irregularities. Overall, we show that rhythmic categories are a valuable tool for gait kinematic analysis and that they can be used to quantify temporal patterns in the motor domain.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="lynch5ConundrumEsophagogastric2025" class="col-sm-10"> <div class="title">Issue Information</div> <div class="author"> </div> <div class="periodical"> <em>Annals of the New York Academy of Sciences</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1111/nyas.15023" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div id="manriquezSpikingNeuralNetwork2025" class="col-sm-10"> <div class="title">A Spiking Neural Network for Investigation Auditory Rhythm Processing: 8th International Conference on Auditory Cortex</div> <div class="author"> Rodrigo Manriquez, Sonja A. Kotz, <em>Andrea Ravignani</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Bart De Boer' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 8th International Conference on Auditory Cortex</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>While artificial intelligence and deep learning models have gained significant attention in neuroscience, there is still a need for more biologically realistic models to capture the dynamics of specific brain functions. Spiking Neural Networks (SNNs) may fill this gap by using the precise timing of spikes as the main mechanism for information processing, offering greater biological plausibility. In this study, we put forward a spiking neural network framework for auditory rhythm processing, focusing on its detection and representation.Our approach involves encoding auditory signals into spike trains, using a biologically inspired subcortical model of sound processing. This model simulates peripheral auditory functions, particularly the auditory transduction that occurs at the cochlear level, by reproducing auditory nerve responses tuned to specific characteristic frequencies. For each characteristic frequency, a spike train is generated. In this way, raw acoustic waveforms are converted into a temporally precise spiking representation that preserves key temporal features of the input. The encoded spiking data is then processed using a spiking autoencoder, a neural architecture designed to learn efficient representations of the input. The autoencoder is trained to reconstruct the amplitude envelope of the acoustic signal at its output layer, effectively capturing rhythmic and amplitude modulations present in the original sound. Through our simulations, we can demonstrate that when the network is trained on isochronous rhythmic sequences, i.e. acoustic sequences where time intervals between successive events have the same duration, emergent rhythmic patterns materialize in the latent representations learned by the spiking autoencoder. The network learns the timing of onsets and develops predictive capabilities, allowing for the anticipation of subsequent rhythmic events. This sensitivity reflects a form of temporal expectation encoded within the spike-based architecture.To further investigate the network’s ability to encode rhythmic structures, we evaluate its performance with alternative rhythmic paradigms, such as missing beats and alternating high and low-amplitude pulses. These simulations can reveal the nature of timing representations learned by the network, emphasising how it relies on temporal features of the input to anticipate subsequent pulses. Moreover, our findings support the hypothesis that rhythm encoding can arise from purely spike-based processing, also reinforcing the biological plausibility of SNNs in auditory neuroscience research.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="martinRooksCorvusFrugilegus2025" class="col-sm-10"> <div class="title">Rooks (Corvus Frugilegus) Spontaneously Attempt to Vocally Entrain to Rhythmic Stimuli</div> <div class="author"> Killian Martin, Maëlan Tomasek, Agnès Hivet, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Andrea Ravignani, Nicolas Obin, Valérie Dufour' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.21203/rs.3.rs-6785900/v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Musicality is the predisposition to process and produce music. In human beings, processing and producing music often involves entrainment, the ability to synchronise behaviour to external rhythms. Non-human primates generally exhibit poor entrainment skills, which may be due to their relative lack of vocal learning. Focusing on non-primate species like songbird species, is one way to investigate further the evolution of musicality. Here, we investigate spontaneous vocal entrainment in rooks, a social corvid, using non-biologically relevant stimuli. We exposed individual rooks to non natural sound stimuli and tested the effect of various tempos and metrical structures on their willingness to sing along, and on their capacity to entrain (singing along the tempo or the meter, or both). Several individuals sang while listening to the stimuli. Among them, two individuals were influenced by particular tempi and/or metrical structures: one bird produced shorter vocalisations at slower tempo and another reduced the intervals between its vocalisations upon hearing isochronous sequences with a unary metre and slow tempo. Still, the timing of the start of their vocalisations did not match accurately the timing of the beat of the stimuli. Our results suggest that rooks attempted to vocally entrain, an as-yet rare demonstration of vocal flexibility, even among open-ended vocal learners. Despite their evolutionary distance from humans, rooks, and possibly other corvids and songbirds, are interesting species for future studies on rhythmic perception, and could help shed light on the evolution of musical abilities.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="raimondiRhythmWombBroad2025" class="col-sm-10"> <div class="title">Rhythm in the Womb: A Broad Comparative Approach to Auditory and Motor Entrainment</div> <div class="author"> <em>Teresa Raimondi</em> and <em>Andrea Ravignani</em> </div> <div class="periodical"> <em>Current Anthropology</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>reserved</p> </div> </div> </div> </li> <li> <div class="row"> <div id="savageDoesSynchronisedSinging2025" class="col-sm-10"> <div class="title">Does Synchronised Singing Enhance Social Bonding More than Speaking Does? A Global Experimental Stage 1 Registered Report</div> <div class="author"> Patrick E. Savage, Adwoa Ampiah-Bonney, Aleksandar Arabadjiev, and <span class="more-authors" title="click to view 70 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '70 more authors' ? 'Adwoa Arhine, Juan F. Ariza, Joshua Silberstein Bamford, Brenda Suyanne Barbosa, Ann-Kathrin Beck, Michel Belyk, Emmanouil Benetos, Joseph Bulbulia, Anne Cabildo, Sasha Calhoun, Gakuto Chiba, Stephen Ithel Duran, Ulvhild Færøvik, Tecumseh Fitch, Shinya Fujii, Shira Gabriel, Felix Haiduk, Niels Chr. Hansen, Shantala Hegde, Ferenc Honbolygó, Jiawen Huang, Nori Jacoby, Yannick Jadoul, Zixuan Jia, Taeyun Jung, Csaba Kertész, Uswatun Khasanah, Inkuk Kim, Yoichi Kitayama, Wojciech Krzyżanowski, Urise Kuikuro, Dilyana Kurdova, Pauline Larrouy-Maestri, Juan David Leongómez, Fang Liu, Teona Lomsadze, Psyche Loui, Yiqing Ma, John Mcbride, Dayna Moya, Rogerdison Natsitsabui, Giacomo Novembre, Florence Ewomazino Nweke, Patricia Opondo, Yuto Ozaki, Hineatua Parkinson, Mark Lenini Parselelo, Danya Vivianne Pavlovich, Peter Pfordresher, Katarzyna Pisanski, Piotr Podlipniak, Tudor Popescu, Polina Proutskova, Suzanne Purdy, Andrea Ravignani, Limor Raviv, Dhwani P. Sadaphal, Swayambhu Ratna Shakya, Dor Shilton, Javier Silva-Zurita, Ignacio Soto-Silva, Bronwyn Tarr, Adam Tierney, Prapatsorn Tiratanti, Laurel Trainor, Christina Der Nederlanden, Marco Antonio Correa Varella, Shahaboddin Dabaghi Varnosfaderani, Mason Youngblood, Roberto Zariquiey' : '70 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">70 more authors</span> </div> <div class="periodical"> Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.31234/osf.io/pv3m9_v3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The evolution of music, speech, and sociality have been debated since before Darwin. The social bonding hypothesis proposes that these phenomena may be interlinked: musicality may have facilitated the evolution of social bonding beyond the possibilities of spoken language. Although dozens of experimental studies have argued that synchronised rhythms can promote bonding, methodological issues including publication bias, sample bias, experimenter effects, and appropriateness of experimental controls make it unclear whether synchronous singing reliably and generally enhances bonding relative to speaking. Here, we propose a Registered Report to overcome these issues through a global experiment in diverse languages aiming to collect data from 1800 participants across 60 sites. The social bonding hypothesis predicts that bonding will increase more after synchronous singing than after spoken (sequential) conversation or (simultaneous) recitation, while alternative hypotheses predict that song will not increase bonding relative to speech. Regardless of outcome, these results will provide an unprecedented understanding of cross-cultural relationships between music, speech, and sociality.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="savageProgrammaticStage12025" class="col-sm-10"> <div class="title">A Programmatic Stage 1 Registered Report of Global Song-Speech Relationships Replicating and Extending Ozaki et al.(2024) and Savage et al. 2</div> <div class="author"> Patrick E. Savage and Zixuan Jia </div> <div class="periodical"> <em>databases</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="valenteBioacousticsRhythm2025" class="col-sm-10"> <div class="title">Bioacoustics and Rhythm</div> <div class="author"> Daria Valente and <em>Andrea Ravignani</em> </div> <div class="periodical"> <em>In The Oxford Handbook of Approaches to Language Evolution</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1093/oxfordhb/9780192886491.013.27" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Bioacoustics is an interdisciplinary science; it concerns aspects of communication through sound in animals, including humans. Within language evolution research, bioacoustics is particularly relevant to address questions about the origins and evolution of speech. Areas of bioacoustics research include the perception and production of acoustic signals, their transmission in the environment, and the neural and anatomical correlates that determine and influence communication. Here, we review the tools needed to conduct bioacoustics research, from initial methodological choices to instrument choice, based on critical features needed for data collection in the field. We discuss key methodological choices in sound recording and processing raw data from a data collection campaign. We provide recommendations for the recording and use of acoustic signals. We discuss analysis techniques, focusing exclusively on rhythm in animal vocalizations. We conclude by stressing the potential for multidisciplinary collaborations, and highlighting key areas where these collaborations could happen.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vanderwerffHumansCanFind2025" class="col-sm-10"> <div class="title">Humans Can Find Rhythm in Randomly Timed Sounds</div> <div class="author"> <em>Jelle van der Werff</em>, Tommaso Tufarelli, Laura Verga, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Andrea Ravignani' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Royal Society Open Science</em>, Aug 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1098/rsos.250453" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Humans are keen pattern-seekers and take advantage of regularities present in their environment. In the temporal domain, we may call these patterns rhythms, but what is rhythm? Definitions vary, but all presuppose a categorical distinction between rhythm and randomness. Here, we challenge this view and show that two types of random sound sequences—classically considered arrhythmic by experimenters—differ in the amount of regularity humans reconstruct from them. When asked to synchronize to randomly timed sounds, participants leverage statistics to estimate the underlying tempo of the sequence, similar to linear statistical estimators. Theoretically, our results challenge current definitions of rhythm by showing that rhythmicity and randomness are instances of a continuum. Methodologically, our data and mathematical model show that a common method for creating random timing, namely the jittering of event onsets, introduces an undesirable regularity that humans readily exploit. New experiments should aim to maximize temporal randomness, and past experiments’ outcomes require reconsideration.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div id="biancoNeuralEncodingMusical2024" class="col-sm-10"> <div class="title">Neural Encoding of Musical Expectations in a Non-Human Primate</div> <div class="author"> Roberta Bianco, Nathaniel J. Zuk, Félix Bigand, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Eros Quarta, Stefano Grasso, Flavia Arnese, Andrea Ravignani, Alexandra Battaglia-Mayer, Giacomo Novembre' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Current Biology</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.cub.2023.12.019" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The appreciation of music is a universal trait of humankind.1–3 Evidence supporting this notion includes the ubiquity of music across cultures4–7 and the natural predisposition toward music that humans display early in development.8–10 Are we musical animals because of species-specific predispositions? This question cannot be answered by relying on cross-cultural or developmental studies alone, as these cannot rule out enculturation.11 Instead, it calls for cross-species experiments testing whether homologous neural mechanisms underlying music perception are present in non-human primates. We present music to two rhesus monkeys, reared without musical exposure, while recording electroencephalography (EEG) and pupillometry. Monkeys exhibit higher engagement and neural encoding of expectations based on the previously seeded musical context when passively listening to real music as opposed to shuffled controls. We then compare human and monkey neural responses to the same stimuli and find a species-dependent contribution of two fundamental musical features—pitch and timing12—in generating expectations: while timing- and pitch-based expectations13 are similarly weighted in humans, monkeys rely on timing rather than pitch. Together, these results shed light on the phylogeny of music perception. They highlight monkeys’ capacity for processing temporal structures beyond plain acoustic processing, and they identify a species-dependent contribution of time- and pitch-related features to the neural encoding of musical expectations.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="burchardtToolkitDynamicStudy2024" class="col-sm-10"> <div class="title">A Toolkit for the Dynamic Study of Air Sacs in Siamang and Other Elastic Circular Structures</div> <div class="author"> Lara S. Burchardt, Yana Van De Sande, Mounia Kehy, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Marco Gamba, Andrea Ravignani, Wim Pouw' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>PLOS Computational Biology</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1371/journal.pcbi.1012222" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Biological structures are defined by rigid elements, such as bones, and elastic elements, like muscles and membranes. Computer vision advances have enabled automatic tracking of moving animal skeletal poses. Such developments provide insights into complex time-varying dynamics of biological motion. Conversely, the elastic soft-tissues of organisms, like the nose of elephant seals, or the buccal sac of frogs, are poorly studied and no computer vision methods have been proposed. This leaves major gaps in different areas of biology. In primatology, most critically, the function of air sacs is widely debated; many open questions on the role of air sacs in the evolution of animal communication, including human speech, remain unanswered. To support the dynamic study of soft-tissue structures, we present a toolkit for the automated tracking of semi-circular elastic structures in biological video data. The toolkit contains unsupervised computer vision tools (using Hough transform) and supervised deep learning (by adapting DeepLabCut) methodology to track inflation of laryngeal air sacs or other biological spherical objects (e.g., gular cavities). Confirming the value of elastic kinematic analysis, we show that air sac inflation correlates with acoustic markers that likely inform about body size. Finally, we present a pre-processed audiovisual-kinematic dataset of 7+ hours of closeup audiovisual recordings of siamang ( Symphalangus syndactylus ) singing. This toolkit ( https://github.com/WimPouw/AirSacTracker ) aims to revitalize the study of non-skeletal morphological structures across multiple species.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="degregorioIsochronousSinging32024" class="col-sm-10"> <div class="title">Isochronous Singing in 3 Crested Gibbon Species ( \emphNomascus Spp.)</div> <div class="author"> Chiara De Gregorio, <em>Teresa Raimondi</em>, Valeria Bevilacqua, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Chiara Pertosa, Daria Valente, Filippo Carugati, Francesca Bandoli, Livio Favaro, Brice Lefaux, Andrea Ravignani, Marco Gamba' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>Current Zoology</em>, Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1093/cz/zoad029" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract The search for common characteristics between the musical abilities of humans and other animal species is still taking its first steps. One of the most promising aspects from a comparative point of view is the analysis of rhythmic components, which are crucial features of human communicative performance but also well-identifiable patterns in the vocal displays of other species. Therefore, the study of rhythm is becoming essential to understand the mechanisms of singing behavior and the evolution of human communication. Recent findings provided evidence that particular rhythmic structures occur in human music and some singing animal species, such as birds and rock hyraxes, but only 2 species of nonhuman primates have been investigated so far (Indri indri and Hylobates lar). Therefore, our study aims to consistently broaden the list of species studied regarding the presence of rhythmic categories. We investigated the temporal organization in the singing of 3 species of crested gibbons (Nomascus gabriellae, Nomascus leucogenys, and Nomascus siki) and found that the most prominent rhythmic category was isochrony. Moreover, we found slight variation in songs’ tempo among species, with N. gabriellae and N. siki singing with a temporal pattern involving a gradually increasing tempo (a musical accelerando), and N. leucogenys with a more regular pattern. Here, we show how the prominence of a peak at the isochrony establishes itself as a shared characteristic in the small apes considered so far.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="degregorioIsochronyAncestralCondition2024" class="col-sm-10"> <div class="title">Isochrony as Ancestral Condition to Call and Song in a Primate</div> <div class="author"> Chiara De Gregorio, Marco Maiolini, <em>Teresa Raimondi</em>, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Filippo Carugati, Longondraza Miaretsoa, Daria Valente, Valeria Torti, Cristina Giacoma, Andrea Ravignani, Marco Gamba' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>Annals of the New York Academy of Sciences</em>, Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/nyas.15151" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Animal songs differ from calls in function and structure, and have comparative and translational value, showing similarities to human music. Rhythm in music is often distributed in quantized classes of intervals known as rhythmic categories. These classes have been found in the songs of a few nonhuman species but never in their calls. Are rhythmic categories song-specific, as in human music, or can they transcend the song–call boundary? We analyze the vocal displays of one of the few mammals producing both songs and call sequences: Indri indri . We test whether rhythmic categories (a) are conserved across songs produced in different contexts, (b) exist in call sequences, and (c) differ between songs and call sequences. We show that rhythmic categories occur across vocal displays. Vocalization type and function modulate deployment of categories. We find isochrony (1:1 ratio, like the rhythm of a ticking clock) in all song types, but only advertisement songs show three rhythmic categories (1:1, 1:2, 2:1 ratios). Like songs, some call types are also isochronous. Isochrony is the backbone of most indri vocalizations, unlike human speech, where it is rare. In indri, isochrony underlies both songs and hierarchy-less call sequences and might be ancestral to both.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="duengenAnecdotalObservationsSocially2024" class="col-sm-10"> <div class="title">Anecdotal Observations of Socially Learned Vocalizations in Harbor Seals</div> <div class="author"> Diandra Duengen, Martin Polotzek, Eoin O’Sullivan, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Andrea Ravignani' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Animal Behavior and Cognition</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.26451/abc.11.04.04.2024" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Harbor seals (Phoca vitulina) are more solitary than many other pinnipeds. Yet, they are capable of vocal learning, a form of social learning. Most extant literature examines social animals when investigating social learning, despite sociality not being a prerequisite. Here, we report two formerly silent harbor seals who initiated vocalizations, after having repeatedly observed a conspecific receiving food rewards for vocalizing. Our observations suggest both social and vocal learning in a group of captive harbor seals, a species that lives semi-solitarily in the wild. We propose that, in this case, social learning acted as a shortcut to acquiring food rewards compared to the comparatively costly asocial learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="duengenVocalUsageLearning2024" class="col-sm-10"> <div class="title">Vocal Usage Learning and Vocal Comprehension Learning in Harbor Seals</div> <div class="author"> Diandra Duengen, <em>Yannick Jadoul</em>, and <em>Andrea Ravignani</em> </div> <div class="periodical"> <em>BMC Neuroscience</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1186/s12868-024-00899-4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Background Which mammals show vocal learning abilities, e.g., can learn new sounds, or learn to use sounds in new contexts? Vocal usage and comprehension learning are submodules of vocal learning. Specifically, vocal usage learning is the ability to learn to use a vocalization in a new context; vocal comprehension learning is the ability to comprehend a vocalization in a new context. Among mammals, harbor seals ( Phoca vitulina ) are good candidates to investigate vocal learning. Here, we test whether harbor seals are capable of vocal usage and comprehension learning. Results We trained two harbor seals to (i) switch contexts from a visual to an auditory cue. In particular, the seals first produced two vocalization types in response to two hand signs; they then transitioned to producing these two vocalization types upon the presentation of two distinct sets of playbacks of their own vocalizations. We then (ii) exposed the seals to a combination of trained and novel vocalization stimuli. In a final experiment, (iii) we broadcasted only novel vocalizations of the two vocalization types to test whether seals could generalize from the trained set of stimuli to only novel items of a given vocal category. Both seals learned all tasks and took\,≤ 16 sessions to succeed across all experiments. In particular, the seals showed contextual learning through switching the context from former visual to novel auditory cues, vocal matching and generalization. Finally, by responding to the played-back vocalizations with distinct vocalizations, the animals showed vocal comprehension learning. Conclusions It has been suggested that harbor seals are vocal learners; however, to date, these observations had not been confirmed in controlled experiments. Here, through three experiments, we could show that harbor seals are capable of both vocal usage and comprehension learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="goncharovaHarbourSealsCan2024" class="col-sm-10"> <div class="title">Harbour Seals Can Articulate to Modulate Formants</div> <div class="author"> Maria V. Goncharova, <em>Yannick Jadoul</em>, Colleen Reichmuth, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'W. Tecumseh Fitch, Andrea Ravignani' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In The IMPRS Conference 2024</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="goncharovaVocalTractDynamics2024" class="col-sm-10"> <div class="title">Vocal Tract Dynamics Shape the Formant Structure of Conditioned Vocalizations in a Harbor Seal</div> <div class="author"> Maria Goncharova, <em>Yannick Jadoul</em>, Colleen Reichmuth, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'W. Tecumseh Fitch, Andrea Ravignani' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Annals of the New York Academy of Sciences</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/nyas.15189" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Formants, or resonance frequencies of the upper vocal tract, are an essential part of acoustic communication. Articulatory gestures—such as jaw, tongue, lip, and soft palate movements—shape formant structure in human vocalizations, but little is known about how nonhuman mammals use those gestures to modify formant frequencies. Here, we report a case study with an adult male harbor seal trained to produce an arbitrary vocalization composed of multiple repetitions of the sound wa . We analyzed jaw movements frame-by-frame and matched them to the tracked formant modulation in the corresponding vocalizations. We found that the jaw opening angle was strongly correlated with the first (F1) and, to a lesser degree, with the second formant (F2). F2 variation was better explained by the jaw angle opening when the seal was lying on his back rather than on the belly, which might derive from soft tissue displacement due to gravity. These results show that harbor seals share some common articulatory traits with humans, where the F1 depends more on the jaw position than F2. We propose further in vivo investigations of seals to further test the role of the tongue on formant modulation in mammalian sound production.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="hartmannDelineatingFieldLanguage2024" class="col-sm-10"> <div class="title">Delineating the Field of Language Evolution Research: A Quantitative Analysis of Peer-Review Patterns at the Joint Conference on Language Evolution (JCoLE 2022)</div> <div class="author"> Stefan Hartmann, Sławomir Wacewicz, <em>Andrea Ravignani</em>, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Daria Valente, Evelina Daniela Rodrigues, Rie Asano, Yannick Jadoul' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Interaction Studies. Social Behaviour and Communication in Biological and Artificial Systems</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1075/is.00024.har" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Research on language evolution is an established subject area yet permeated by terminological controversies about which topics should be considered pertinent to the field and which not. By consequence, scholars focusing on language evolution struggle in providing precise demarcations of the discipline, where even the very central notions of evolution and language are elusive. We aimed at providing a data-driven characterisation of language evolution as a field of research by relying on quantitative analysis of data drawn from 697 reviews on 255 submissions from the Joint Conference on Language Evolution 2022 (Kanazawa, Japan). Our results delineate a field characterized by a core of main research topics such as iconicity, sign language, multimodality. Despite being explored within the framework of language evolution research, only very recently these topics became popular in linguistics. As a result, language evolution has the potential to emerge as a forefront of linguistic research, bringing innovation to the study of language. We also see the emergence of more recent topics like rhythm, music, and vocal learning. Furthermore, the community identifies cognitive science, primatology, archaeology, palaeoanthropology, and genetics as key areas, encouraging empirical rather than theoretical work. With new themes, models, and methodologies emerging, our results depict an intrinsically multidisciplinary and evolving research field, likely adapting as language itself.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="hershCetaceansAreNext2024" class="col-sm-10"> <div class="title">Cetaceans Are the next Frontier for Vocal Rhythm Research</div> <div class="author"> Taylor A. Hersh, <em>Andrea Ravignani</em>, and Hal Whitehead </div> <div class="periodical"> <em>Proceedings of the National Academy of Sciences</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1073/pnas.2313093121" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>While rhythm can facilitate and enhance many aspects of behavior, its evolutionary trajectory in vocal communication systems remains enigmatic. We can trace evolutionary processes by investigating rhythmic abilities in different species, but research to date has largely focused on songbirds and primates. We present evidence that cetaceans—whales, dolphins, and porpoises—are a missing piece of the puzzle for understanding why rhythm evolved in vocal communication systems. Cetaceans not only produce rhythmic vocalizations but also exhibit behaviors known or thought to play a role in the evolution of different features of rhythm. These behaviors include vocal learning abilities, advanced breathing control, sexually selected vocal displays, prolonged mother–infant bonds, and behavioral synchronization. The untapped comparative potential of cetaceans is further enhanced by high interspecific diversity, which generates natural ranges of vocal and social complexity for investigating various evolutionary hypotheses. We show that rhythm (particularly isochronous rhythm, when sounds are equally spaced in time) is prevalent in cetacean vocalizations but is used in different contexts by baleen and toothed whales. We also highlight key questions and research areas that will enhance understanding of vocal rhythms across taxa. By coupling an infraorder-level taxonomic assessment of vocal rhythm production with comparisons to other species, we illustrate how broadly comparative research can contribute to a more nuanced understanding of the prevalence, evolution, and possible functions of rhythm in animal communication.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="jadoulParselmouthBioacousticsAutomated2023" class="col-sm-10"> <div class="title">Parselmouth for Bioacoustics: Automated Acoustic Analysis in Python</div> <div class="author"> <em>Yannick Jadoul</em>, Bart De Boer, and <em>Andrea Ravignani</em> </div> <div class="periodical"> <em>Bioacoustics</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1080/09524622.2023.2259327" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Bioacoustics increasingly relies on large datasets and computational methods. The need to batch-process large amounts of data and the increased focus on algorithmic processing require software tools. To optimally assist in a bioacoustician’s workflow, software tools need to be as simple and effective as possible. Five years ago, the Python package Parselmouth was released to provide easy and intuitive access to all functionality in the Praat software. Whereas Praat is principally designed for phonetics and speech processing, plenty of bioacoustics studies have used its advanced acoustic algorithms. Here, we evaluate existing usage of Parselmouth and discuss in detail several studies which used the software library. We argue that Parselmouth has the potential to be used even more in bioacoustics research, and suggest future directions to be pursued with the help of Parselmouth.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="kakiharaLearningTransmissionRhythmic2024" class="col-sm-10"> <div class="title">Learning and Transmission of Rhythmic Information Is Associated with Working Memory and Sensorimotor Synchronization Skill: The Neurosciences and Music VIII</div> <div class="author"> Marcelo Kakihara, Mathilde Martin, <em>Jelle van der Werff</em>, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Andrea Ravignani, Maria Celeste Fasano, Morten Storm Overgaard, Peter Keller, Massimo Lumaca' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In The Neurosciences and Music VIII: Wiring, Re-Wiring, and Well-Being</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="lameiraRecursiveSelfembeddedVocal2024" class="col-sm-10"> <div class="title">Recursive Self-Embedded Vocal Motifs in Wild Orangutans</div> <div class="author"> Adriano R Lameira, Madeleine E Hardus, <em>Andrea Ravignani</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Teresa Raimondi, Marco Gamba' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>eLife</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.7554/eLife.88348" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Recursive procedures that allow placing a vocal signal inside another of a similar kind provide a neuro-computational blueprint for syntax and phonology in spoken language and human song. There are, however, no known vocal sequences among nonhuman primates arranged in self-embedded patterns that evince vocal recursion or potential incipient or evolutionary transitional forms thereof, suggesting a neuro-cognitive transformation exclusive to humans. Here, we uncover that wild flanged male orangutan long calls feature rhythmically isochronous call sequences nested within isochronous call sequences, consistent with two hierarchical strata. Remarkably, three temporally and acoustically distinct call rhythms in the lower stratum were not related to the overarching rhythm at the higher stratum by any low multiples, which suggests that these recursive structures were neither the result of parallel non-hierarchical procedures nor anatomical artifacts of bodily constraints or resonances. Findings represent a case of temporally recursive hominid vocal combinatorics in the absence of syntax, semantics, phonology, or music. Second-order combinatorics, ‘sequences within sequences’, involving hierarchically organized and cyclically structured vocal sounds in ancient hominids may have preluded the evolution of recursion in modern language-able humans. , Language is the most powerful communication tool known in nature. By combining a finite set of elements, it allows us to encode infinite messages. This enables communication about virtually anything, from alerting others to potential dangers, to recommending a favourite book. The prevailing theory of the last 70 years suggests that this ability rests on a computational process in the brain that is unique to humans, known as recursion. Recursion enables humans to produce and place a language element or pattern of elements inside another element or pattern of the same kind. In this way, a clause can be embedded inside another ‘carrier’ clause to extend a thought, argument, or scenario, for example, “the dog, which chased the cat, was barking”. While recursion offers a simple, yet potent, explanation for the endless possibilities of language, how and why recursion – and by extension language – emerged in humans but no other animals remains a mystery. Lameira et al. observed vocal patterns in wild orangutans that appeared to be composed of different elements. As orangutans and other great apes are our closest living relatives, they represent the most realistic model for studying the ability of human ancestors to use and comprehend language. Therefore, Lameira et al. set out to determine if this was a case of vocal patterning embedded within a similar vocal pattern, which could indicate that recursion underpins production of these calls. Analysing recordings of long calls made by wild male orangutans showed that they are organized as two layers, where calls with a regular beat (or tempo) are produced within another “carrier” call of a different tempo. Up to three different call types, each with their own signature tempo, can occur within the same carrier call. Further analysis confirmed these call types were unrelated to the carrier. The findings of Lameira et al. demonstrate that orangutans produce recursive vocal sequences that could represent a possible precursor to recursion in humans, offering a potential avenue for studying how recursion, and ultimately language, evolved in humans. In the future, better understanding of how language evolved may help to refine machine learning algorithms that aim to recognize, predict or generate text.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="leonettiCrossspeciesFrameworkClassifying2024" class="col-sm-10"> <div class="title">A Cross-Species Framework for Classifying Sound-Movement Couplings</div> <div class="author"> Silvia Leonetti, <em>Andrea Ravignani</em>, and Wim Pouw </div> <div class="periodical"> <em>Neuroscience &amp; Biobehavioral Reviews</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1016/j.neubiorev.2024.105911" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div id="leonettiWhyDogsWag2024" class="col-sm-10"> <div class="title">Why Do Dogs Wag Their Tails?</div> <div class="author"> Silvia Leonetti, Giulia Cimarelli, Taylor A. Hersh, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Andrea Ravignani' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Biology Letters</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1098/rsbl.2023.0407" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Tail wagging is a conspicuous behaviour in domestic dogs ( Canis familiaris ). Despite how much meaning humans attribute to this display, its quantitative description and evolutionary history are rarely studied. We summarize what is known about the mechanism, ontogeny, function and evolution of this behaviour. We suggest two hypotheses to explain its increased occurrence and frequency in dogs compared to other canids. During the domestication process, enhanced rhythmic tail wagging behaviour could have (i) arisen as a by-product of selection for other traits, such as docility and tameness, or (ii) been directly selected by humans, due to our proclivity for rhythmic stimuli. We invite testing of these hypotheses through neurobiological and ethological experiments, which will shed light on one of the most readily observed yet understudied animal behaviours. Targeted tail wagging research can be a window into both canine ethology and the evolutionary history of characteristic human traits, such as our ability to perceive and produce rhythmic behaviours.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="lumacaFrontoparietalNetworkTopology2024" class="col-sm-10"> <div class="title">Frontoparietal Network Topology as a Neural Marker of Musical Perceptual Abilities</div> <div class="author"> M. Lumaca, P. E. Keller, G. Baggio, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'V. Pando-Naude, C. J. Bajada, M. A. Martinez, J. H. Hansen, A. Ravignani, N. Joe, P. Vuust, K. Vulić, K. Sandberg' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em>Nature Communications</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1038/s41467-024-52479-z" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Why are some individuals more musical than others? Neither cognitive testing nor classical localizationist neuroscience alone can provide a complete answer. Here, we test how the interplay of brain network organization and cognitive function delivers graded perceptual abilities in a distinctively human capacity. We analyze multimodal magnetic resonance imaging, cognitive, and behavioral data from 200+ participants, focusing on a canonical working memory network encompassing prefrontal and posterior parietal regions. Using graph theory, we examine structural and functional frontoparietal network organization in relation to assessments of musical aptitude and experience. Results reveal a positive correlation between perceptual abilities and the integration efficiency of key frontoparietal regions. The linkage between functional networks and musical abilities is mediated by working memory processes, whereas structural networks influence these abilities through sensory integration. Our work lays the foundation for future investigations into the neurobiological roots of individual differences in musicality.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="maldarelliChicksProduceConsonant2024" class="col-sm-10"> <div class="title">Chicks Produce Consonant, Sometimes Jazzy, Sounds</div> <div class="author"> Gianmarco Maldarelli, Andrea Dissegna, <em>Andrea Ravignani</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Cinzia Chiandetti' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Biology Letters</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1098/rsbl.2024.0374" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Several animal species prefer consonant over dissonant sounds, a building block of musical scales and harmony. Could consonance and dissonance be linked, beyond music, to the emotional valence of vocalizations? We extracted the fundamental frequency from calls of young chickens with either positive or negative emotional valence, i.e. contact, brood and food calls. For each call, we calculated the frequency ratio between the maximum and the minimum values of the fundamental frequency, and we investigated which frequency ratios occurred with higher probability. We found that, for all call types, the most frequent ratios matched perfect consonance, like an arpeggio in pop music. These music-like intervals, based on the auditory frequency resolution of chicks, cannot be miscategorized into contiguous dissonant intervals. When we analysed frequency ratio distributions at a finer-grained level, we found some dissonant ratios in the contact calls produced during distress only, thus sounding a bit jazzy. Complementing the empirical data, our computational simulations suggest that physiological constraints can only partly explain both consonances and dissonances in chicks’ phonation. Our data add to the mounting evidence that the building blocks of human musical traits can be found in several species, even phylogenetically distant from us.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="manriquezpBioacousticClassificationSmall2024" class="col-sm-10"> <div class="title">Bioacoustic Classification of a Small Dataset of Mammalian Vocalisations Using Deep Learning</div> <div class="author"> Rodrigo Manriquez P, Sonja A. Kotz, <em>Andrea Ravignani</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Bart De Boer' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Bioacoustics</em>, Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1080/09524622.2024.2354468" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div id="osieckaIsochronyBarksCape2024" class="col-sm-10"> <div class="title">Isochrony in Barks of Cape Fur Seal ( \emphArctocephalus\emph Pusillus Pusillus ) Pups and Adults</div> <div class="author"> Anna N. Osiecka, Jack Fearey, <em>Andrea Ravignani</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Lara S. Burchardt' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Ecology and Evolution</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1002/ece3.11085" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract Animal vocal communication often relies on call sequences. The temporal patterns of such sequences can be adjusted to other callers, follow complex rhythmic structures or exhibit a metronome-like pattern (i.e., isochronous). How regular are the temporal patterns in animal signals, and what influences their precision? If present, are rhythms already there early in ontogeny? Here, we describe an exploratory study of Cape fur seal ( Arctocephalus pusillus pusillus ) barks—a vocalisation type produced across many pinniped species in rhythmic, percussive bouts. This study is the first quantitative description of barking in Cape fur seal pups. We analysed the rhythmic structures of spontaneous barking bouts of pups and adult females from the breeding colony in Cape Cross, Namibia. Barks of adult females exhibited isochrony, that is they were produced at fairly regular points in time. Instead, intervals between pup barks were more variable, that is skipping a bark in the isochronous series occasionally. In both age classes, beat precision, that is how well the barks followed a perfect template, was worse when barking at higher rates. Differences could be explained by physiological factors, such as respiration or arousal. Whether, and how, isochrony develops in this species remains an open question. This study provides evidence towards a rhythmic production of barks in Cape fur seal pups and lays the groundwork for future studies to investigate the development of rhythm using multidimensional metrics.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="ozakiGloballySongsInstrumental2024" class="col-sm-10"> <div class="title">Globally, Songs and Instrumental Melodies Are Slower and Higher and Use More Stable Pitches than Speech: A Registered Report</div> <div class="author"> Yuto Ozaki, Adam Tierney, Peter Q. Pfordresher, and <span class="more-authors" title="click to view 72 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '72 more authors' ? 'John M. McBride, Emmanouil Benetos, Polina Proutskova, Gakuto Chiba, Fang Liu, Nori Jacoby, Suzanne C. Purdy, Patricia Opondo, W. Tecumseh Fitch, Shantala Hegde, Martín Rocamora, Rob Thorne, Florence Nweke, Dhwani P. Sadaphal, Parimal M. Sadaphal, Shafagh Hadavi, Shinya Fujii, Sangbuem Choo, Marin Naruse, Utae Ehara, Latyr Sy, Mark Lenini Parselelo, Manuel Anglada-Tort, Niels Chr. Hansen, Felix Haiduk, Ulvhild Færøvik, Violeta Magalhães, Wojciech Krzyżanowski, Olena Shcherbakova, Diana Hereld, Brenda Suyanne Barbosa, Marco Antonio Correa Varella, Mark Van Tongeren, Polina Dessiatnitchenko, Su Zar Zar, Iyadh El Kahla, Olcay Muslu, Jakelin Troy, Teona Lomsadze, Dilyana Kurdova, Cristiano Tsope, Daniel Fredriksson, Aleksandar Arabadjiev, Jehoshaphat Philip Sarbah, Adwoa Arhine, Tadhg Ó Meachair, Javier Silva-Zurita, Ignacio Soto-Silva, Neddiel Elcie Muñoz Millalonco, Rytis Ambrazevičius, Psyche Loui, Andrea Ravignani, Yannick Jadoul, Pauline Larrouy-Maestri, Camila Bruder, Tutushamum Puri Teyxokawa, Urise Kuikuro, Rogerdison Natsitsabui, Nerea Bello Sagarzazu, Limor Raviv, Minyu Zeng, Shahaboddin Dabaghi Varnosfaderani, Juan Sebastián Gómez-Cañón, Kayla Kolff, Christina Vanden Bosch Der Nederlanden, Meyha Chhatwal, Ryan Mark David, I. Putu Gede Setiawan, Great Lekakul, Vanessa Nina Borsan, Nozuko Nguqu, Patrick E. Savage' : '72 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">72 more authors</span> </div> <div class="periodical"> <em>Science Advances</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1126/sciadv.adm9797" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Both music and language are found in all known human societies, yet no studies have compared similarities and differences between song, speech, and instrumental music on a global scale. In this Registered Report, we analyzed two global datasets: (i) 300 annotated audio recordings representing matched sets of traditional songs, recited lyrics, conversational speech, and instrumental melodies from our 75 coauthors speaking 55 languages; and (ii) 418 previously published adult-directed song and speech recordings from 209 individuals speaking 16 languages. Of our six preregistered predictions, five were strongly supported: Relative to speech, songs use (i) higher pitch, (ii) slower temporal rate, and (iii) more stable pitches, while both songs and speech used similar (iv) pitch interval size and (v) timbral brightness. Exploratory analyses suggest that features vary along a “musi-linguistic” continuum when including instrumental melodies and recited lyrics. Our study provides strong empirical evidence of cross-cultural regularities in music and speech. , Global collaboration by 75 researchers finds acoustic relationships between speech, song, and instrumental music across cultures.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="rossiTestingRhythmicAbilities2024" class="col-sm-10"> <div class="title">Testing Rhythmic Abilities in Developmental Dyslexia</div> <div class="author"> Marina Rossi, Eline A. Smit, <em>Jelle van der Werff</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Andrea Ravignani, Tamara Rathcke' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Music Perception: An Interdisciplinary Journal</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1525/mp.2024.42.2.135" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Rhythm processing deficits in developmental dyslexia (DD) span across different rhythmic subcomponents and are difficult to capture using one experimental paradigm. How are dyslexic deficits related to motor periodicity, i.e., the execution of repetitive actions while internally generating rhythm? The present experiment investigated rhythm production in DD by means of unprompted tapping paradigm, testing the hypothesis that the ability to internally generate rhythmic patterns may be impaired. The tasks involved tapping of isochronous sequences at a comfortable and a fast tempo and tapping of a free rhythm. Forty adolescents diagnosed with DD (with or without comorbid dyscalculia) participated, along with thirty typically developing control participants. A background questionnaire gathered information about participants’ prior music training. The data show that both dyslexic groups tapped faster than the typically developing participants at the comfortable tempo. We found no statistical differences between groups in fast isochronous tapping or in the free rhythm production tasks, irrespective of music training or the presence of dyscalculia. All participants favored regular rhythms when tapping a free rhythm, with a notable preference for isochrony. These results have theoretical and clinical implications for rhythm deficit hypotheses of DD.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="vanderwerffThebeatPythonPackage2024" class="col-sm-10"> <div class="title">Thebeat: A Python Package for Working with Rhythms and Other Temporal Sequences</div> <div class="author"> <em>Jelle van der Werff</em>, <em>Andrea Ravignani</em>, and <em>Yannick Jadoul</em> </div> <div class="periodical"> <em>Behavior Research Methods</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3758/s13428-023-02334-8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Abstract thebeat is a Python package for working with temporal sequences and rhythms in the behavioral and cognitive sciences, as well as in bioacoustics. It provides functionality for creating experimental stimuli, and for visualizing and analyzing temporal data. Sequences, sounds, and experimental trials can be generated using single lines of code. thebeat contains functions for calculating common rhythmic measures, such as interval ratios, and for producing plots, such as circular histograms. thebeat saves researchers time when creating experiments, and provides the first steps in collecting widely accepted methods for use in timing research. thebeat is an open-source, on-going, and collaborative project, and can be extended for use in specialized subfields. thebeat integrates easily with the existing Python ecosystem, allowing one to combine our tested code with custom-made scripts. The package was specifically designed to be useful for both skilled and novice programmers. thebeat provides a foundation for working with temporal sequences onto which additional functionality can be built. This combination of specificity and plasticity should facilitate research in multiple research contexts and fields of study.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div id="raimondiIsochronyRhythmicInteraction2023" class="col-sm-10"> <div class="title">Isochrony and Rhythmic Interaction in Ape Duetting</div> <div class="author"> <em>Teresa Raimondi</em>, Giovanni Di Panfilo, Matteo Pasquali, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Martina Zarantonello, Livio Favaro, Tommaso Savini, Marco Gamba, Andrea Ravignani' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Proceedings of the Royal Society B: Biological Sciences</em>, Jan 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1098/rspb.2022.2244" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>How did rhythm originate in humans, and other species? One cross-cultural universal, frequently found in human music, is isochrony: when note onsets repeat regularly like the ticking of a clock. Another universal consists in synchrony (e.g. when individuals coordinate their notes so that they are sung at the same time). An approach to biomusicology focuses on similarities and differences across species, trying to build phylogenies of musical traits. Here we test for the presence of, and a link between, isochrony and synchrony in a non-human animal. We focus on the songs of one of the few singing primates, the lar gibbon ( Hylobates lar ), extracting temporal features from their solo songs and duets. We show that another ape exhibits one rhythmic feature at the core of human musicality: isochrony. We show that an enhanced call rate overall boosts isochrony, suggesting that respiratory physiological constraints play a role in determining the song’s rhythmic structure. However, call rate alone cannot explain the flexible isochrony we witness. Isochrony is plastic and modulated depending on the context of emission: gibbons are more isochronous when duetting than singing solo. We present evidence for rhythmic interaction: we find statistical causality between one individual’s note onsets and the co-singer’s onsets, and a higher than chance degree of synchrony in the duets. Finally, we find a sex-specific trade-off between individual isochrony and synchrony. Gibbon’s plasticity for isochrony and rhythmic overlap may suggest a potential shared selective pressure for interactive vocal displays in singing primates. This pressure may have convergently shaped human and gibbon musicality while acting on a common neural primate substrate. Beyond humans, singing primates are promising models to understand how music and, specifically, a sense of rhythm originated in the primate phylogeny.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div id="giomoProcessingLearningTemporal2022" class="col-sm-10"> <div class="title">The Processing and Learning of Temporal Patterns: From Behaviour to Computational Models</div> <div class="author"> <em>Dunia Giomo</em> </div> <div class="periodical"> <em></em> Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>open</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div id="zaniniEffectsAnimacyProcessing2020" class="col-sm-10"> <div class="title">Effects of Animacy on the Processing of Morphological Number: A Cognitive Inheritance?</div> <div class="author"> Chiara Zanini, Rosa Rugani, <em>Dunia Giomo</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Francesca Peressotti, Francesca Franzon' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Word Structure</em>, Mar 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3366/word.2020.0158" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Language encodes into morphology part of the information present in the referential world. Some features are marked in the great majority of languages, such as the numerosity of the referents that is encoded in morphological Number. Other features do not surface as frequently in morphological markings, yet they are pervasive in natural languages. This is the case of animacy, that can ground Gender systems as well as constrain the surfacing of Number. The diffusion of numerosity and animacy could mirror their biological salience at the extra-linguistic cognitive level. Human extra-linguistic numerical abilities are phylogenetically ancient and are observed in non-human animal species, especially when counting salient animate entities such as social companions. Does the saliency of animacy influence the morphological encoding of Number in language processing? We designed an experiment to test the encoding of morphological Number in language processing in relation to animacy. In Italian, Gender and Number are mandatorily expressed in a fusional morpheme. In some nouns denoting animate referents, Gender encodes the sex of referents and is semantically interpretable. In some other animate nouns and in inanimate nouns, Gender is uninterpretable at the semantic level. We found that it is easier to inflect for Number nouns when the inflectional morpheme is interpretable with respect to a semantic feature related to animacy. We discuss the possibility that the primacy of animacy in counting is mirrored in morphological processing and that morphology is designed to easily express information that is salient from a cognitive point of view.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div id="giomoSensorimotorSynchronizationTemporal2019" class="col-sm-10"> <div class="title">Sensorimotor Synchronization and Temporal Order Judgements Reveal Saccadic Temporal Recalibration</div> <div class="author"> <em>Dunia Giomo</em>, Brent Parsons, and Domenica Bueti </div> <div class="periodical"> <em>In PERCEPTION</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div id="parsonsSaccadicTemporalRecalibration2018" class="col-sm-10"> <div class="title">Saccadic Temporal Recalibration Alters Action and Perception</div> <div class="author"> Brent Parsons, <em>Dunia Giomo</em>, and Domenica Bueti </div> <div class="periodical"> <em>Journal of Vision</em>, Sep 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1167/18.10.1003" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div id="franzonEffectsAnimacyProcessing2017" class="col-sm-10"> <div class="title">Effects of Animacy on the Processing of Morphological Number: A Cognitive Inheritance? A Psycholinguistic Study</div> <div class="author"> Francesca Franzon, Rosa Rugani, <em>Dunia Giomo</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Chiara Zanini' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In BOOK OF ABSTRACTS</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="franzonInterferenceAnimacyProcessing2017" class="col-sm-10"> <div class="title">The Interference of Animacy in the Processing of Morphological Number</div> <div class="author"> F. Franzon, C. Zanini, D. Giomo, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'F. Peressotti, R. Rugani' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 10th International Morphological Processing Conference (MoProc)</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>none</p> </div> </div> </div> </li> <li> <div class="row"> <div id="ravignaniVisualizingInterpretingRhythmic2017" class="col-sm-10"> <div class="title">Visualizing and Interpreting Rhythmic Patterns Using Phase Space Plots</div> <div class="author"> <em>Andrea Ravignani</em> </div> <div class="periodical"> <em>Music Perception</em>, Jun 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1525/mp.2017.34.5.557" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Structure in musical rhythm can be measured using a number of analytical techniques. While some techniques—like circular statistics or grammar induction—rely on strong top-down assumptions, assumption-free techniques can only provide limited insights on higher-order rhythmic structure. I suggest that research in music perception and performance can benefit from systematically adopting phase space plots, a visualization technique originally developed in mathematical physics that overcomes the aforementioned limitations. By jointly plotting adjacent interonset intervals (IOI), the motivic rhythmic structure of musical phrases, if present, is visualized geometrically without making any a priori assumptions concerning isochrony, beat induction, or metrical hierarchies. I provide visual examples and describe how particular features of rhythmic patterns correspond to geometrical shapes in phase space plots. I argue that research on music perception and systematic musicology stands to benefit from this descriptive tool, particularly in comparative analyses of rhythm production. Phase space plots can be employed as an initial assumption-free diagnostic to find higher order structures (i.e., beyond distributional regularities) before proceeding to more specific, theory-driven analyses.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 animal behavior, computational bioacoustics, brains and cognition . </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>